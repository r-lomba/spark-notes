{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESERCIZIO SUGLI RDD (RESILIENT DISTRIBUTED DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPERAZIONI PRELIMINARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARICHIAMO UN FILE DI TESTO DI ESEMPIO IN \"/tmp/corsospark\" SIA SU HDFS CHE SU FILESYSTEM LOCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /tmp/corsospark\n",
    "hdfs dfs -mkdir /tmp/corsospark\n",
    "hdfs dfs -put example_rdd.txt /tmp/corsospark\n",
    "cp example_rdd.txt /tmp/corsospark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LANCIAMO ORA LA SPARK CLI E FACCIAMO QUALCHE TEST CON GLI RDD (RESILIENT DISTRIBUTED DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARALLELIZZIAMO UN RDD USANDO LO SPARK CONTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_RDD = sc.parallelize(range(10), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTROLLIAMO LE RDD PARTITIONS - IN QUESTO CASO ACCORPIAMO TUTTI I DATI DELLE PARTIZIONI RDD SUL DRIVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTROLLIAMO LE RDD PARTITIONS - IN QUESTO CASO MANTENIAMO GLI SPLIT DELLE PARTIZIONI CREATE DALLA PARALLELIZZAZIONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_RDD.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROVIAMO A LEGGERE UN TEXT FILE DAL LOCAL FILESYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_RDD = sc.textFile(\"file:///tmp/corsospark/example_rdd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROVIAMO A MOSTRARE LA PRIMA RIGA DEL FILE CHE ABBIAMO TENTATO DI CARICARE. IL COMANDO CON OGNI PROBABILITA' FALLIRA' PERCHE' NON SAPPIAMO SU QUALE NODO STA GIRANDO IL CONTAINER DEL NOSTRO WORKER!\n",
    "\n",
    "### E' IMPORTANTE RICORDARE CHE MOLTO SPESSO QUANDO SI OTTENGONO ERRORI FACENDO RIFERIMENTO A RISORSE LOCALI SI PUO' RISPARMIARE MOLTO TEMPO DI DEBUGGING TENTANDO DI IMMAGINARE SE STIAMO RICADENDO IN QUESTO CASO.\n",
    "\n",
    "### AD ESEMPIO, SE IL NOSTRO PROGRAMMA STA FACENDO USO DI UNA LIBRERIA PYTHON NON DI USO COMUNE, PUO' ESSERE PROBABILE CHE IL PROBLEMA SIA CHE LA LIBRERIA STESSA NON SIA STATA INSTALLATA SU *** TUTTI *** I NODI DEL CLUSTER CON \"pip install\". MAGARI CI SI E' DIMENTICATI DI INSTALLARLA SU UN SOLO NODO, E IL 90% DELLE VOLTE IL PROGRAMMA GIRA CON SUCCESSO PERCHE' NON VENGONO DISTRIBUITI WORKER CONTAINER SU QUEL NODO.\n",
    "\n",
    "### MA QUANDO ACCADE OTTENIAMO DELLE ECCEZIONI E MAGARI NON CAPIAMO IL MOTIVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RIPROVIAMO CON IL FILE CHE AVEVAMO PRECEDENTEMENTE CARICATO IN HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_RDD = sc.textFile(\"hdfs:///tmp/corsospark/example_rdd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTA VOLTA RIUSCIREMO A VISUALIZZARE IL CONTENUTO DEL FILE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNA PRIMA SEMPLICE APPLICAZIONE DEGLI RDD IN MAPREDUCE: WORD COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTE DI MAP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINIAMO DUE HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words(line):\n",
    "\treturn line.split()\n",
    "\n",
    "def create_pair(word):\n",
    "\treturn (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESTRAIAMO DELLE KEY VALUE PAIRS USANDO MAPREDUCE AFFIANCANDO AD OGNI PAROLA (KEY) UN VALORE (VALUE) CHE NEL NOSTRO CASO SARA' \"1\". USEREMO POI QUESTO VALORE PER TOTALIZZARE IL NUMERO DI PAROLE IN UN TESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_RDD = text_RDD.flatMap(split_words).map(create_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESEGUIAMO FISICAMENTE LA RACCOLTA DEI DATI TRAMITE UNA ACTION DI \"collect\" CHE ESEGUE IL GRAFO DI ESECUZIONE DEI COMANDI MAP CHE ABBIAMO IMPOSTATO COME PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTE DI REDUCE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINIAMO UNA ULTERIORE HELPER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_counts(a, b):\n",
    "\treturn a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A PARITA' DI CHIAVE (PAROLA) FACCIAMO REDUCE DEI VALORI (PER OGNI PAROLA E' UN \"1\" CHE SI SOMMA) USANDO LA NOSTRA HELPER FUNCTION CHE LAVORA SU TUPLE DI DUE ELEMENTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_RDD = pairs_RDD.reduceByKey(sum_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESEGUIAMO LA ACTION DI COLLECT, ESEGUIAMO L'EXECUTION GRAPH E VERIFICHIAMO L'AVVENUTA TOTALIZZAZIONE A PARITA' DI PAROLA. IN QUESTO CASO, E' SOLO LA PAROLA \"far\" CHE VIENE CONTEGGIATA DUE VOLTE. QUESTO E' CORRETTO PERCHE' AD ESEMPIO \"A\" ED \"a\" SONO DIVERSE A CAUSA DELLA CASE SENSITIVITY E LA RIMANENZA DELLE PAROLE E' COSTITUITA DA ELEMENTI UNICI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESERCIZIO SUI DATAFRAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPERAZIONI PRELIMINARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SU FILESYSTEM LOCALE, COME UTENTE \"hdfs\", CREIAMO UNA DIRECTORY DI LAVORO E COPIAMO AL SUO INTERNO I FILES CSV DELLE TABELLE DI TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir corsospark\n",
    "cd corsospark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COPIARE IN \"corsospark\" I SEGUENTI FILE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.csv\n",
    "customers.csv\n",
    "employees.csv\n",
    "employee_territories.csv\n",
    "order_details.csv\n",
    "orders.csv\n",
    "products.csv\n",
    "regions.csv\n",
    "shippers.csv\n",
    "suppliers.csv\n",
    "territories.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTRIAMO IN HIVE E CREIAMO LE TABELLE DI ESEMPIO CHE ANDREMO AD USARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE DATABASE spark;\n",
    "USE spark;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS categories (\n",
    "categoryID int,\n",
    "categoryName String,\n",
    "description String,\n",
    "picture String)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS customers (\n",
    "customerID String,\n",
    "companyName String,\n",
    "contactName String,\n",
    "contactTitle String,\n",
    "address\tcity String,\n",
    "region String,\n",
    "postalCode String,\n",
    "country String,\n",
    "phone String,\n",
    "fax String)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS employee_territories (\n",
    "employeeID String,\n",
    "territoryID String)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS employees (\n",
    "employeeID int,\n",
    "lastName String,\n",
    "firstName String,\n",
    "title String,\n",
    "titleOfCourtesy String,\n",
    "birthDate String,\n",
    "hireDate String,\n",
    "address String,\n",
    "city String,\n",
    "region String,\n",
    "postalCode String,\n",
    "country String,\n",
    "homePhone String,\n",
    "extension int,\n",
    "photo String,\n",
    "notes String,\n",
    "reportsTo int,\n",
    "photoPath String)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS order_details (\n",
    "orderID int,\n",
    "productID int,\n",
    "unitPrice Double,\n",
    "quantity int,\n",
    "discount Double)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS orders (\n",
    "orderID int,\n",
    "customerID String,\n",
    "employeeID int,\n",
    "orderDate String,\n",
    "requiredDate String,\n",
    "shippedDate String,\n",
    "shipVia int,\n",
    "freight Double,\n",
    "shipName String,\n",
    "shipAddress String,\n",
    "shipCity String,\n",
    "shipRegion String,\n",
    "shipPostalCode String,\n",
    "shipCountry String)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS products (\n",
    "productID int,\n",
    "productName String,\n",
    "supplierID int,\n",
    "categoryID int,\n",
    "quantityPerUnit String,\n",
    "unitPrice Double,\n",
    "unitsInStock int,\n",
    "unitsOnOrder int,\n",
    "reorderLevel int,\n",
    "discontinued int)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS regions (\n",
    "regionID int,\n",
    "regionDescription String)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS shippers (\n",
    "shipperID int,\n",
    "companyName String,\n",
    "phone String)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS suppliers (\n",
    "supplierID int,\n",
    "companyName String,\n",
    "contactName String,\n",
    "contactTitle String,\n",
    "address String,\n",
    "city String,\n",
    "region String,\n",
    "postalCode String,\n",
    "country String,\n",
    "phone String,\n",
    "fax String,\n",
    "homePage String)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS territories (\n",
    "territoryID int,\n",
    "territoryDescription String,\n",
    "regionID int)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VERIFICHIAMO LA CORRETTA CREAZIONE DELLE TABELLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show tables;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TORNIAMO AL PROMPT DI SISTEMA OPERATIVO E CARICHIAMO ALCUNI FILE CSV DI ESEMPIO IN \"/tmp/corsospark\" SU HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -mkdir /tmp/corsospark\n",
    "\n",
    "hdfs dfs -put categories.csv /tmp/corsospark\n",
    "hdfs dfs -put customers.csv /tmp/corsospark\n",
    "hdfs dfs -put employee_territories.csv /tmp/corsospark\n",
    "hdfs dfs -put employees.csv /tmp/corsospark\n",
    "hdfs dfs -put order_details.csv /tmp/corsospark\n",
    "hdfs dfs -put orders.csv /tmp/corsospark\n",
    "hdfs dfs -put products.csv /tmp/corsospark\n",
    "hdfs dfs -put regions.csv /tmp/corsospark\n",
    "hdfs dfs -put shippers.csv /tmp/corsospark\n",
    "hdfs dfs -put suppliers.csv /tmp/corsospark\n",
    "hdfs dfs -put territories.csv /tmp/corsospark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TORNIAMO NELLA HIVE SHELL E POPOLIAMO LE TABELLE \"employee\" ECC. CON I DATI PRESENTI NEI FILE DI ESEMPIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD DATA INPATH 'hdfs:///tmp/corsospark/categories.csv'\n",
    "OVERWRITE INTO TABLE categories;\n",
    "\n",
    "LOAD DATA INPATH 'hdfs:///tmp/corsospark/customers.csv'\n",
    "OVERWRITE INTO TABLE customers;\n",
    "\n",
    "LOAD DATA INPATH 'hdfs:///tmp/corsospark/employee_territories.csv'\n",
    "OVERWRITE INTO TABLE employee_territories;\n",
    "\n",
    "LOAD DATA INPATH 'hdfs:///tmp/corsospark/employees.csv'\n",
    "OVERWRITE INTO TABLE employees;\n",
    "\n",
    "LOAD DATA INPATH 'hdfs:///tmp/corsospark/order_details.csv'\n",
    "OVERWRITE INTO TABLE order_details;\n",
    "\n",
    "LOAD DATA INPATH 'hdfs:///tmp/corsospark/orders.csv'\n",
    "OVERWRITE INTO TABLE orders;\n",
    "\n",
    "LOAD DATA INPATH 'hdfs:///tmp/corsospark/products.csv'\n",
    "OVERWRITE INTO TABLE products;\n",
    "\n",
    "LOAD DATA INPATH 'hdfs:///tmp/corsospark/regions.csv'\n",
    "OVERWRITE INTO TABLE regions;\n",
    "\n",
    "LOAD DATA INPATH 'hdfs:///tmp/corsospark/shippers.csv'\n",
    "OVERWRITE INTO TABLE shippers;\n",
    "\n",
    "LOAD DATA INPATH 'hdfs:///tmp/corsospark/suppliers.csv'\n",
    "OVERWRITE INTO TABLE suppliers;\n",
    "\n",
    "LOAD DATA INPATH 'hdfs:///tmp/corsospark/territories.csv'\n",
    "OVERWRITE INTO TABLE territories;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VERIFICHIAMO L'AVVENUTO CARICAMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select * from categories;\n",
    "\n",
    "select * from customers;\n",
    "\n",
    "select * from employee_territories;\n",
    "\n",
    "select * from employees;\n",
    "\n",
    "select * from order_details;\n",
    "\n",
    "select * from orders;\n",
    "\n",
    "select * from products;\n",
    "\n",
    "select * from regions;\n",
    "\n",
    "select * from shippers;\n",
    "\n",
    "select * from suppliers;\n",
    "\n",
    "select * from territories;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LANCIAMO ORA LA SPARK CLI E FACCIAMO QUALCHE TEST CON I DATAFRAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTRIAMO IN PYSPARK (CON DRIVER IPYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYSPARK_DRIVER_PYTHON=ipython pyspark \\\n",
    "--executor-memory 2G \\\n",
    "--executor-cores 2 \\\n",
    "--num-executors 4 \\\n",
    "--driver-memory 4G \\\n",
    "--master=yarn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTIAMO ALCUNE LIBRERIE - N.B. FARE COPIA/INCOLLA E ALLA FINE DARE DUE VOLTE INVIO POICHE' STAMO USANDO IPYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import getopt\n",
    "import unicodedata\n",
    "import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREIAMO ALCUNI DATAFRAME A PARTIRE DALLE TABELLE HIVE APPENA CREATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_DF = sqlContext.table(\"spark.categories\")\n",
    "customers_DF = sqlContext.table(\"spark.customers\")\n",
    "employee_territories_DF = sqlContext.table(\"spark.employee_territories\")\n",
    "employees_DF = sqlContext.table(\"spark.employees\")\n",
    "order_details_DF = sqlContext.table(\"spark.order_details\")\n",
    "orders_DF = sqlContext.table(\"spark.orders\")\n",
    "products_DF = sqlContext.table(\"spark.products\")\n",
    "regions_DF = sqlContext.table(\"spark.regions\")\n",
    "shippers_DF = sqlContext.table(\"spark.shippers\")\n",
    "suppliers_DF = sqlContext.table(\"spark.suppliers\")\n",
    "territories_DF = sqlContext.table(\"spark.territories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSSIAMO VISUALIZZARE LO SCHEMA DI UN DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSSIAMO CONTARE I RECORDS IN UN DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALIZZIAMO ALCUNI RECORD DI ESEMPIO DAI NOSTRI DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_DF.show(5)\n",
    "orders_DF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSSIAMO CACHARE I NOSTRI DATAFRAME IN MODO CHE LE SUCCESSIVE ACTION (COME PURE LE GENERICHE DML, O LE JOIN CON ATRE TABELLE) RISULTINO PIU' VELOCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINIAMO UN FILTRO (EQUIVALENTE DELLA WHERE) SENZA TRIGGERARE IMMEDIATAMENTE LA COLLECTION DEI RECORD. USIAMO UNA VARIABILE DI APPOGGIO PER FARE UN ESEMPIO PIO' VARIEGATO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = 'France'\n",
    "\n",
    "orders_filered_DF = orders_DF \\\n",
    ".filter(\"shipcountry = '\" + destination + \"'\")\n",
    "\n",
    "orders_filered_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSSIAMO ANCHE ESEGUIRE TUTTA LA PIPELINE DEL COMANDO IN UNA SOLA VOLTA SCATENANDO IMMEDIATAMENTE LA ACTION DI \"show\" O DI \"collect\" O DI \"take\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF \\\n",
    ".filter(\"shipcountry = '\" + destination + \"'\") \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSSIAMO ANCHE ESEGUIRE UNA AZIONE DI \"collect\" O DI \"take\" DI ALCUNI RECORDS FARE UN TEST PER VEDERE LE DIFFERENZE (FONDAMENTALMENTE LA FORMATTAZIONE DELL'OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF \\\n",
    ".filter(\"shipcountry = '\" + destination + \"'\") \\\n",
    ".collect()\n",
    "\n",
    "orders_DF \\\n",
    ".filter(\"shipcountry = '\" + destination + \"'\") \\\n",
    ".take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SI PUO' FARE ANCHE UN FILTRO PIU' COMPLESSO CON SINTASSI SQL-LIKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annoInizio = '1996'\n",
    "meseInizio = '07'\n",
    "annoFine = '1996'\n",
    "meseFine = '07'\n",
    "\n",
    "orders_DF \\\n",
    ".filter(\"(substr(shippeddate,1,10) between ' \" + annoInizio + \"-\" + meseInizio + \"-01' \\\n",
    "\t\tand '\" + annoFine + \"-\" + meseFine + \"-31') \\\n",
    "\t\tand \\\n",
    "\t\tshipvia='2' \") \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSSIAMO ANCHE RENDERE L'ESEMPIO PIU' \"PYTHONICO\" E MENO \"SQL LEGACY LIKE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF \\\n",
    ".filter(orders_DF.shipcountry == destination) \\\n",
    ".show()\n",
    "\n",
    "orders_DF \\\n",
    ".filter( \\\n",
    "\t\t(orders_DF.shipcountry == destination) & \\\n",
    "\t\t(orders_DF.shippeddate.contains(annoInizio + '-' + meseInizio)) \\\n",
    "\t\t) \\\n",
    ".show()\n",
    "\n",
    "orders_DF \\\n",
    ".filter( \\\n",
    "\t\t(orders_DF.shipcountry == destination) & \\\n",
    "\t\t(orders_DF.shippeddate.like(annoInizio + '-' + meseInizio + '%')) \\\n",
    "\t\t) \\\n",
    ".show()\n",
    "\n",
    "orders_DF \\\n",
    ".filter( \\\n",
    "\t\t(orders_DF.shipcountry == destination) & \\\n",
    "\t\t(orders_DF.shippeddate.like(annoInizio + '-' + meseInizio + '%')) \\\n",
    "\t\t) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPPURE ANCHE USANDO UNA LISTA COME RANGE DI RIFERIMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = ['1996-07-01',  '1996-07-31']\n",
    "\n",
    "orders_DF \\\n",
    ".filter( \\\n",
    "\t\t(orders_DF.shipcountry == destination) & \\\n",
    "\t\t(orders_DF.shippeddate.between(*date_range)) \\\n",
    "\t\t) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INVECE DI USARE LA LISTA POSSIAMO SPECIFICARE IL RANGE DI RIFERIMENTO ESPLICITAMENTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF \\\n",
    ".filter( \\\n",
    "\t\t(orders_DF.shipcountry == destination) & \\\n",
    "\t\t(orders_DF.shippeddate.between('1996-07-01',  '1996-07-31')) \\\n",
    "\t\t) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A VOLTE E' UTILE REFERENZIARE LA COLONNA DIRETTAMENTE. IMPORTANTE: QUI CI SERVONO LE SQLFUNCTIONS, CHE ABBIAMO GIA' IMPORTATO ALL'INIZIO, MA RIPETO LA IMPORT PER CHIAREZZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "orders_DF \\\n",
    ".filter(col('shipcountry') == destination) \\\n",
    ".filter(col('shippeddate').between('1996-07-01',  '1996-07-31')) \\\n",
    ".show()\n",
    "\n",
    "orders_DF \\\n",
    ".filter(col('shipcountry').isin(['Italy','France'])) \\\n",
    ".filter(col('shippeddate').between('1996-07-01',  '1996-10-31')) \\\n",
    ".show()\n",
    "\n",
    "list_of_countries = ['Italy','France']\n",
    "startDate = annoInizio +'-' + meseInizio + '01'\n",
    "endDate = '1996-08-31'\n",
    "\n",
    "orders_DF \\\n",
    ".filter(col('shipcountry').isin(list_of_countries)) \\\n",
    ".filter(col('shippeddate').between(startDate,  endDate)) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SI PUO' USARE UNA CLAUSOLA WHERE IN MODO PIU' \"PYTHONICO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF \\\n",
    ".where(col('shipcountry').isin(['Italy','France']) & \\\n",
    "\t\t(col('shippeddate').between('1996-07-01',  '1996-10-31'))) \\\n",
    ".show()\n",
    "\n",
    "orders_DF \\\n",
    ".where(col('shipregion').isNotNull() & \\\n",
    "\t\t(col('shippeddate').between('1996-07-01',  '1996-10-31'))) \\\n",
    ".show()\n",
    "\n",
    "orders_DF \\\n",
    ".where(col('shipregion').isNull() & \\\n",
    "\t\t(col('shippeddate').between('1996-07-01',  '1996-10-31'))) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESEMPIO DI NEGAZIONE (NOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF \\\n",
    ".where(~ (col('shipregion').isNull()) & \\\n",
    "\t\t(col('shippeddate').between('1996-07-01',  '1996-10-31'))) \\\n",
    ".show()\n",
    "\n",
    "orders_DF \\\n",
    ".where(~ (col('shipregion') == 'NULL') & \\\n",
    "\t\t(col('shippeddate').between('1996-07-01',  '1996-10-31')) | \\\n",
    "\t\t(col('shippeddate') == '1996-10-15')) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NON DIMENTICHIAMOCI PERO' CHE COME SPIEGATO ALL'INIZIO POSSIAMO FARE LE COSE PIU' FACILI ANCHE SE A VOLTE QUESTO PUO' INFICIARE LE PERFORMANCE, PERCHE' SI PASSA TRAMITE UN OTTIMIZZATORE DEL QUALE NON ABBIAMO IL PIENO CONTROLLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF \\\n",
    ".where(\"shipcountry = '\" + destination + \"' AND shippeddate between '1996-07-01' AND '1996-07-31'\") \\\n",
    ".show()\n",
    "\n",
    "orders_DF \\\n",
    ".where(\"shipcountry = '\" + destination + \"' AND shippeddate like '1996%'\") \\\n",
    ".show()\n",
    "\n",
    "orders_DF \\\n",
    ".where(\"shipcountry = '\" + destination + \"' AND shippeddate ='1996-11-28 00:00:00.000'\") \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSSIAMO ORA AGGIUNGERE ALLA NOSTRA PIPELINE DI COMANDI UNA SELECT SOLO DI ALCUNI CAMPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF \\\n",
    ".where(col('shippeddate').between('1996-07-01',  '1996-10-31')) \\\n",
    ".select(['orderid','customerid', 'shippeddate', 'shipvia', 'shipcity', 'shipcountry']) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROVIAMO A CONTARE I RECORDS DEL NOSTRO DATAFRAME DI ORDINI FILTRATI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filered_DF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESCE UN ERRORE! PERCHE'? PERCHE' \"show\" NON ASSEGNA ALCUN RECORD ALL'OGGETTO A SINISTRA BENSI' VISUALIZZA SOLTANTO! PROVIAMO INVECE A FARE UN ASSEGNAMENTO NEL MODO CORRETTO OVVERO SENZA LA \"show\" E SUCCESSIVAMENTE RIPROVIAMO A FARE LA COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filered_DF = orders_DF \\\n",
    ".where(col('shippeddate').between('1996-07-01',  '1996-10-31')) \\\n",
    ".select(['orderid','customerid', 'shippeddate', 'freight', 'shipvia', 'shipcity', 'shipcountry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RIPROVIAMO LA COUNT, CHE STAVOLTA FUNZIONERA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filered_DF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORA POSSIAMO VERIFICARE CHE IL DATAFRAME ORIGINALE CONTINUA A CONTENERE LO STESSO NUMERO DI RECORDS DELLA TABELLA ORIGINALE. SONO ORA DUE OGGETTI DISTINTI CHE CONTINUANO A VIVERE DI VITA PROPRIA. LI POTREMO RIUSARE SEPARATAMENTE A VALLE NEL NOSTRO PROGRAMMA, E UN OGGETTO NON INTERFERIRA' CON L'ALTRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSSIAMO FARE \"show\" DEI RECORDS NEL NOSTRO NUOVO OGGETTO DATAFRAME CON SOLO ALCUNE COLONNE, GIA' FILTRATE ECC ECC, E CHE VOLENDO POTREMO ULTERIORMENTE RI-FILTRARE A VALLE, OPPURE METTERE IN JOIN CON ALTRE TABELLE, IL TUTTO, COME DETTO, SENZA INTERFERIRE CON \"orders_DF\" OVVERO IL NOSTRO DATAFRAME ORIGINALE. LA CALL A \"show\" DI DEFAULT RESTITUISCE 20 RECORDS, PROVIAMO A RACCOGLIERE PIU' RECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filered_DF.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROVIAMO ORA A RAGGRUPPARE E SOMMARE IL COSTO DELLA SPEDIZIONE \"freight\" PER NAZIONE. SELEZIONIAMO QUINDI DAL NOSTRO DATAFRAME PRECEDENTE GIA' FILTRATO PER DATA SOLO I CAMPI CHE CI INTERESSANO E EFFETTUIAMO IL GROUPBY E LA SOMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_freight_cost_by_nation_DF = orders_filered_DF \\\n",
    ".select(['shipcountry', 'freight']) \\\n",
    ".groupby(['shipcountry']) \\\n",
    ".sum()\n",
    "\n",
    "orders_freight_cost_by_nation_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VEDIAMO CHE LA COLONNA RAGGRUPPATA ASSUME UN NOME STRANO E SCOMODO DA MANEGGIARE. IN QUESTI CASI SPESSO E' UTILE UTILIZZARE UN ALIAS ANZI, SPESSO QUESTA E' UNA PRATICA DA ESEGUIRE QUASI AD OGNI SELECT, PERCHE' IN CASO DI JOIN CON ALTRI DATAFRAME FORMATI DA CAMPI CON NOMI UGUALI SI POTREBBE FACILMENTE CADERE IN CONFLITTI DI AMBIGUITA'. CERTAMENTE NULLA DI GRAVE, CI SAREBBE UN'ECCEZIONE CHE SEGNALEREBBE IL PROBLEMA, COMUNQUE QUI DI SEGUITO UN PRIMO MODO CON CUI E' POSSIBILE ASSEGNARE ALIAS ALLE COLONNE\n",
    "\n",
    "### NEL COMANDO SUCCESSIVO NOTARE ANCHE CHE ABBIAMO SPEZZATO SU DUE RIGHE L'ELENCO DEI CAMPI OGGETTO DI SELECT (CHE NEGLI ESEMPI PRECEDENTI AVEVAMO CONDENSATO IN UN'UNICA LISTA) E CHE AD UNO SOLO DI QUESTI CAMPI SPEZZATI SU PIU' RIGHE ASSEGNAMO UN ALIAS. QUESTO PERCHE' L'ALTRO CAMPO SARA' OGGETTO DI RAGGRUPPAMENTO E DOVREMO ASSEGNARGLI UN ALIAS ALLA FINE DELLA PIPELINE, COME SI NOTA, OVVERO QUANDO LA \"sum\" SARA' GIA' STATA PROCESSATA\n",
    "\n",
    "### SI NOTI ANCHE CHE NELLA GROUPBY ORA CI STIAMO RIFERENDO AL CAMPO CHE PRIMA SI CHIAMAVA \"shipcountry\" CHIAMANDOLO GIA' CON IL SUO ALIAS \"destination_country\" CHE GLI ABBIAMO ASSEGNATO NELLA SEZIONE PRECEDENTE DELLA PIPELINE DI COMANDI ALTRIMENTI, REFERENZIANDOLO COL NOME ORIGINALE, OTTERREMMO UNA ECCEZIONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_freight_cost_by_nation_DF = orders_filered_DF \\\n",
    ".select(orders_filered_DF['shipcountry'].alias('destination_country'), \\\n",
    "\t\torders_filered_DF['freight']) \\\n",
    ".groupby(['destination_country']) \\\n",
    ".sum() \\\n",
    ".withColumnRenamed(\"sum(freight)\", \"tot_freight\")\n",
    "\n",
    "orders_freight_cost_by_nation_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSSIAMO ORA PER ESEMPIO AGGIUNGERE UNA ULTERIORE COLONNA NUMERICA CON VALORE FISSO \"1\" AL NOSTRO DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_freight_cost_by_nation_01_DF = orders_freight_cost_by_nation_DF \\\n",
    ".withColumn('just_one', lit(1))\n",
    "\n",
    "orders_freight_cost_by_nation_01_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSSIAMO AGGIUNGERE UNA ULTERIORE COLONNA STRINGA CON VALORE FISSO \"Stringa a caso\" AL NOSTRO DATAFRAME. ALLO STESSTO TEMPO AGGIUNGO ANCHE UN INCREMENTALE PSEUDO-RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_freight_cost_by_nation_02_DF = orders_freight_cost_by_nation_01_DF\\\n",
    ".withColumn('just_stringa', lit(\"Stringa a caso\")) \\\n",
    ".withColumn(\"just_incremental\", monotonically_increasing_id())\n",
    "\n",
    "orders_freight_cost_by_nation_02_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A QUESTO PUNTO POSSIAMO SPERIMENTARE UNA ULTERIORE VERSIONE DELLA GROUPBY CON FUNZIONI DI AGGREGAZIONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_freight_cost_by_nation_03_DF = orders_freight_cost_by_nation_02_DF \\\n",
    ".select(orders_freight_cost_by_nation_02_DF['just_stringa'], \\\n",
    "\t\torders_freight_cost_by_nation_02_DF['tot_freight'], \\\n",
    "\t\torders_freight_cost_by_nation_02_DF['just_one'], \\\n",
    "\t\torders_freight_cost_by_nation_02_DF['just_incremental'], \\\n",
    "\t\t) \\\n",
    ".groupby(['just_stringa']) \\\n",
    ".agg({\"tot_freight\": \"avg\", \\\n",
    "\t\t\"just_one\": \"sum\", \\\n",
    "\t\t\"just_incremental\": \"max\"}) \\\n",
    ".withColumnRenamed(\"avg(tot_freight)\", \"avg_freight\") \\\n",
    ".withColumnRenamed(\"sum(just_one)\", \"tot_just_one\") \\\n",
    ".withColumnRenamed(\"max(just_incremental)\", \"max_just_incremental\")\n",
    "\n",
    "orders_freight_cost_by_nation_03_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSSIAMO FARE UN ORDINAMENTO IN DIVERSI MODI (QUI RIPRENDIAMO IL DATAFRAME \"orders_filered_DF\" E FACCIAMO SOLO \"show\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filered_DF \\\n",
    ".orderBy([\"shipcountry\"], ascending=[1]) \\\n",
    ".show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STESSA COSA PER PIU' CAMPI CON DIVERSI ORDINAMENTI ASC/DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filered_DF \\\n",
    ".orderBy([\"shipcountry\", \"shipcity\", \"freight\"], ascending=[1,1,0]) \\\n",
    ".show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINIAMO ORA UNA UDF (USER DEFINED FUNCTION) MOLTO SEMPLICE CHE CI PERMETTE DI PORTARE UNA STRINGA IN MAIUSCOLO OVVIAMENTE PER FARE CIO' SI POTREBBERO USARE \"upper()\" MA E' SOLO PER SPIEGARE LE UDF.\n",
    "\n",
    "### IMPORTANTE: BISOGNA AVERE IMPORTATO LA LIBRERIA \"types\" NOI ALL'INIZIO L'ABBIAMO GIA' FATTO MA LO RISCRIVO PER CHIAREZZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "def porta_in_maiuscolo(input_string):\n",
    "\toutput_string = input_string.upper()\n",
    "\t\n",
    "\treturn(output_string)\n",
    "\t\n",
    "porta_in_maiuscolo_UDF = udf(porta_in_maiuscolo, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USIAMO LA UDF PER PORTARE IN MAIUSCOLO ALCUNI CAMPI DEL DATAFRAME \"orders_filered_DF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filered_uppercase_DF = orders_filered_DF \\\n",
    ".select(orders_filered_DF['orderid'], \\\n",
    "\t\torders_filered_DF['customerid'], \\\n",
    "\t\torders_filered_DF['shippeddate'], \\\n",
    "\t\torders_filered_DF['freight'], \\\n",
    "\t\torders_filered_DF['shipvia'], \\\n",
    "\t\tporta_in_maiuscolo_UDF(orders_filered_DF['shipcity']), \\\n",
    "\t\tporta_in_maiuscolo_UDF(orders_filered_DF['shipcountry'])) \\\n",
    ".withColumnRenamed(\"porta_in_maiuscolo(shipcity)\", \"shipcity\") \\\n",
    ".withColumnRenamed(\"porta_in_maiuscolo(shipcountry)\", \"shipcountry\")\n",
    "\n",
    "orders_filered_uppercase_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINIAMO UN'ALTRA UDF CHE CI PERMETTA AD ESEMPIO DI AGGIUNGERE UNA COLONNA AL NOSTRO DATAFRAME COME VALORE DERIVATO DA ALTRE DUE COLONNE NUMERICHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def somma_due_colonne(input_number_01, input_number_02):\n",
    "\toutput_number = input_number_01 + input_number_02\n",
    "\t\n",
    "\treturn(output_number)\n",
    "\t\n",
    "somma_due_colonne_UDF = udf(somma_due_colonne, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USIAMO LA UDF PER AGGIUNGERE UNA COLONNA DERIVATA AL DATAFRAME \"orders_filered_uppercase_DF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filered_uppercase_somma_DF = orders_filered_uppercase_DF \\\n",
    ".select(orders_filered_uppercase_DF['orderid'], \\\n",
    "\t\torders_filered_uppercase_DF['customerid'], \\\n",
    "\t\torders_filered_uppercase_DF['shippeddate'], \\\n",
    "\t\torders_filered_uppercase_DF['freight'], \\\n",
    "\t\torders_filered_uppercase_DF['shipvia'], \\\n",
    "\t\torders_filered_uppercase_DF['shipcity'], \\\n",
    "\t\torders_filered_uppercase_DF['shipcountry']) \\\n",
    ".withColumn(\"just_somma\", somma_due_colonne_UDF(orders_filered_uppercase_DF['freight'], orders_filered_uppercase_DF['shipvia']))\n",
    "\n",
    "orders_filered_uppercase_somma_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FACCIAMO UNA JOIN TRA DUE TABELLE - PRIMO METODO, METTIAMO UN PO' DI COMPLESSITA' CON UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filered_uppercase_somma_join_shippers_DF = orders_filered_uppercase_somma_DF \\\n",
    ".join(shippers_DF, \\\n",
    "     (orders_filered_uppercase_somma_DF['shipvia'] == shippers_DF['shipperid']), \\\n",
    "\"left_outer\",) \\\n",
    ".select(orders_filered_uppercase_somma_DF['orderid'], \\\n",
    "\t\torders_filered_uppercase_somma_DF['customerid'], \\\n",
    "\t\torders_filered_uppercase_somma_DF['shippeddate'], \\\n",
    "\t\torders_filered_uppercase_somma_DF['freight'], \\\n",
    "\t\tporta_in_maiuscolo_UDF(shippers_DF['companyname']).alias('shipper'), \\\n",
    "\t\tshippers_DF['phone'].alias('shipper_phone'), \\\n",
    "\t\torders_filered_uppercase_somma_DF['shipcity'], \\\n",
    "\t\torders_filered_uppercase_somma_DF['shipcountry'], \\\n",
    "\t\torders_filered_uppercase_somma_DF['just_somma'])\n",
    "\n",
    "orders_filered_uppercase_somma_join_shippers_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FACCIAMO UNA JOIN TRA DUE TABELLE - SECONDO METODO, METTIAMO UN PO' DI COMPLESSITA' CON ALIAS SUI DF IN INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filered_uppercase_somma_join_shippers_DF = orders_filered_uppercase_somma_DF.alias('a') \\\n",
    ".join(shippers_DF.alias('b'),col('b.shipperid') == col('a.shipvia')) \\\n",
    ".select(orders_filered_uppercase_somma_DF['orderid'], \\\n",
    "\t\torders_filered_uppercase_somma_DF['customerid'], \\\n",
    "\t\torders_filered_uppercase_somma_DF['shippeddate'], \\\n",
    "\t\torders_filered_uppercase_somma_DF['freight'], \\\n",
    "\t\tporta_in_maiuscolo_UDF(shippers_DF['companyname']).alias('shipper'), \\\n",
    "\t\tshippers_DF['phone'].alias('shipper_phone'), \\\n",
    "\t\torders_filered_uppercase_somma_DF['shipcity'], \\\n",
    "\t\torders_filered_uppercase_somma_DF['shipcountry'], \\\n",
    "\t\torders_filered_uppercase_somma_DF['just_somma'])\n",
    "\n",
    "orders_filered_uppercase_somma_join_shippers_DF.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRIMO ESEMPIO DI DEDUPLICA DEI RECORD IN QUESTO CASO DEDUPLICHIAMO I RECORD IN MODO DA TENERE GLI \"shipper\" UNIVOCI E QUINDI AGGIUNGIAMO SOLO QUEL CAMPO AL SUBSET DA DEDUPLICARE. OVVIAMENTE IL RISULTATO E' CHE, NON ESSENDO QUESTA UNA GROUP BY ECC ECC, VIENE TENUTO SOLO E SOLTANTO IL PRIMO RECORD DI OGNI \"shipper\" CHE VENGA TROVATO NEL NOSTRO DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filered_uppercase_somma_join_shippers_dropped_DF = \\\n",
    "orders_filered_uppercase_somma_join_shippers_DF \\\n",
    ".dropDuplicates(subset = ['shipper'])\n",
    "\n",
    "orders_filered_uppercase_somma_join_shippers_dropped_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECONDO ESEMPIO DI DEDUPLICA DEI RECORD INDICANDO TUTTI I CAMPI TABELLA NEL SUBSET DA DEDUPLICARE. IN QUESTO MODO OTTERREMO SENZ'ALTRO UN ELENCO DI RECORDS UNIVOCI IN TUTTO E PER TUTTO ALL'INTERNO DEL DATASET. PREPARIAMO QUINDI UNA TABELLA DI COMODO CON DUE CAMPI CON DUPLICAZIONE RECORD E POI FILTRIAMO SULLA TOTALITA' DEI DUE CAMPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filered_uppercase_solodue_DF = orders_filered_uppercase_DF\\\n",
    ".select(['shipcity', 'shipcountry'])\n",
    "\n",
    "orders_filered_uppercase_solodue_DF.show(100)\n",
    "\n",
    "orders_filered_uppercase_solodue_DF.count()\n",
    "\n",
    "orders_filered_uppercase_solodue_dropped_DF = \\\n",
    "orders_filered_uppercase_solodue_DF \\\n",
    ".dropDuplicates(subset = ['shipcity', 'shipcountry'])\n",
    "\n",
    "orders_filered_uppercase_solodue_dropped_DF.show(100)\n",
    "\n",
    "orders_filered_uppercase_solodue_dropped_DF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRIVIAMO UN DATAFRAME COME FILE CSV SU HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF.write.csv('hdfs:///tmp/dump_orders_DF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIENE CREATA UNA DIRECTORY SU HDFS CHE SI CHIAMA COL NOME SPECIFICATO. VERIFICHIAMONE I CONTENUTI DAL PROMT DI SISTEMA OPERATIVO. COME SI PUO' VEDERE, LA DIRECTORY CONTIENE TANTI FILES QUANTI SONO I REDUCERS CHE NOI ABBIAMO IMPOSTATO A LIVELLO DI PARAMETRI SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /tmp/dump_orders_DF\n",
    "\n",
    "Found 3 items\n",
    "-rw-rw-r--   3 hdfs supergroup          0 2020-03-10 18:38 /tmp/dump_orders_DF/_SUCCESS\n",
    "-rw-rw-r--   3 hdfs supergroup      66252 2020-03-10 18:38 /tmp/dump_orders_DF/part-00000-33856ecf-bec6-42a4-961c-41f0d22f5f29-c000.csv\n",
    "-rw-rw-r--   3 hdfs supergroup      66011 2020-03-10 18:38 /tmp/dump_orders_DF/part-00001-33856ecf-bec6-42a4-961c-41f0d22f5f29-c000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RIPROVIAMO QUINDI A SCRIVERE IL FILE USANDO IL PARAMETRO \"coalesce\" OPPURE \"repartition\". LA DIFFERENZA TRA I DUE E' CHE CAMBIANO LE MODALITA' DI SHUFFLE A LIVELLO DI CLUSTER, QUINDI SU GRANDI MOLI DI DATI  LA SCELTA DOVREBBE ESSERE OCULATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF \\\n",
    ".coalesce(1) \\\n",
    ".write.format(\"com.databricks.spark.csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".save(\"hdfs:///tmp/dump_orders_coalesce_DF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VEDIAMO CHE STAVOLTA IL SALVATAGGIO E' STATO ESEGUITO IN UN SOLO FILE FINALE PERCHE' TUTTI I DATI SONO STATI RIAGGREGATI DAL DRIVER PROGRAM, IL QUALE SI E' INFINE OCCUPATO DI SCRIVERLI SU HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /tmp/dump_orders_coalesce_DF\n",
    "\n",
    "Found 2 items\n",
    "-rw-rw-r--   3 hdfs supergroup          0 2020-03-10 18:48 /tmp/dump_orders_coalesce_DF/_SUCCESS\n",
    "-rw-rw-r--   3 hdfs supergroup     132412 2020-03-10 18:48 /tmp/dump_orders_coalesce_DF/part-00000-05de2c34-67c4-4f6f-96f9-ef8a0443bf0d-c000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PER VISUALIZZARE I CONTENUTI DEL FILE CSV, DAL PROMPT DI SISTEMA OPERATIVO, POSSIAMO USARE \"cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /tmp/dump_orders_coalesce_DF/part-00000-05de2c34-67c4-4f6f-96f9-ef8a0443bf0d-c000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFINE, LO STESSO ESEMPIO CON \"repartition\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "orders_DF \\\n",
    ".repartition(1) \\\n",
    ".write.format(\"com.databricks.spark.csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".save(\"hdfs:///tmp/dump_orders_repartition_DF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VERIFICHIAMO SU HDFS L'AVVENUTA AGGREGAZIONE IN UN SOLO FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /tmp/dump_orders_repartition_DF\n",
    "\n",
    "Found 2 items\n",
    "-rw-rw-r--   3 hdfs supergroup          0 2020-03-10 18:51 /tmp/dump_orders_repartition_DF/_SUCCESS\n",
    "-rw-rw-r--   3 hdfs supergroup     132412 2020-03-10 18:51 /tmp/dump_orders_repartition_DF/part-00000-d4ea7922-c8b3-4b2c-bf4f-6f1fefeee71a-c000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VERIFICHIAMO CHE ANCHE IN QUESTO CASO I CONTENUTI DEL FILE SIANO OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /tmp/dump_orders_repartition_DF/part-00000-d4ea7922-c8b3-4b2c-bf4f-6f1fefeee71a-c000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SALVIAMO COME TABELLA HIVE FILETYPE PARQUET NON PARTIZIONATA IN MODALITA' \"Overwrite\". ATTENZIONE: IN QUESTO ESEMPIO LO SCHEMA NAME E' \"spark\" PERCHE' COSI' AVEVAMO CHIAMATO IL  NOSTRO DATABASE DI TEST ALL'INIZIO! SE NON SI TIENE A MENTE QUESTO, IL NOME POTREBBE ESSERE FUORVIANTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF \\\n",
    ".write.mode(\"overwrite\") \\\n",
    ".saveAsTable(\"spark.sample_orders_nopartition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTROLLIAMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_checkrecords_DF = sqlContext.table(\"spark.sample_orders_nopartition\")\n",
    "\n",
    "orders_checkrecords_DF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SALVIAMO ORA IN APPEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF \\\n",
    ".write.mode(\"append\") \\\n",
    ".saveAsTable(\"spark.sample_orders_nopartition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTROLLIAMO NUOVAMENTE IL NUMERO DI RECORDS E NOTIAMO CHE GIUSTAMENTE E' RADDOPPIATO RISPETTO A PRIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_checkrecords_DF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRIVIAMO ORA UNA TABELLA IN FORMATO ORC PARTIZIONATA PER \"shipvia\" CHE NEL NOSTRO CASO E' IL CODICE DELLO SPEDIZIONIERE. ALL'INIZIO USIAMO LA MODALITA' \"overwrite\" PERCHE' LA TABELLA ANCORA NON ESISTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF \\\n",
    ".write \\\n",
    ".mode(\"overwrite\") \\\n",
    ".partitionBy(\"shipvia\") \\\n",
    ".format(\"orc\") \\\n",
    ".saveAsTable(\"spark.sample_orders_partition_orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTROLLIAMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_checkrecords_partition_orc_DF = sqlContext.table(\"spark.sample_orders_partition_orc\")\n",
    "\n",
    "orders_checkrecords_partition_orc_DF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DA PROMPT DI SISTEMA OPERATIVO CONTROLLIAMO E VEDIAMO CHE EFFETTIVAMENTE CI SONO 3 PARTIZIONI CHE CONTENGONO FILES ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/hive/warehouse/spark.db/sample_orders_partition_orc\n",
    "\n",
    "Found 5 items\n",
    "-rw-rw-r--   3 hdfs hive          0 2020-03-10 19:13 /user/hive/warehouse/spark.db/sample_orders_partition_orc/_SUCCESS\n",
    "drwxrwxr-x   - hdfs hive          0 2020-03-10 19:13 /user/hive/warehouse/spark.db/sample_orders_partition_orc/shipvia=1\n",
    "drwxrwxr-x   - hdfs hive          0 2020-03-10 19:13 /user/hive/warehouse/spark.db/sample_orders_partition_orc/shipvia=2\n",
    "drwxrwxr-x   - hdfs hive          0 2020-03-10 19:13 /user/hive/warehouse/spark.db/sample_orders_partition_orc/shipvia=3\n",
    "drwxrwxr-x   - hdfs hive          0 2020-03-10 19:13 /user/hive/warehouse/spark.db/sample_orders_partition_orc/shipvia=__HIVE_DEFAULT_PARTITION\n",
    "\n",
    "hdfs dfs -ls /user/hive/warehouse/spark.db/sample_orders_partition_orc/shipvia=1\n",
    "\n",
    "Found 2 items\n",
    "-rw-rw-r--   3 hdfs hive       9402 2020-03-10 19:13 /user/hive/warehouse/spark.db/sample_orders_partition_orc/shipvia=1/part-00000-76dddaef-2b3e-4717-88a4-2fb4428e62eb.c000.snappy.orc\n",
    "-rw-rw-r--   3 hdfs hive       9166 2020-03-10 19:13 /user/hive/warehouse/spark.db/sample_orders_partition_orc/shipvia=1/part-00001-76dddaef-2b3e-4717-88a4-2fb4428e62eb.c000.snappy.orc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROVIAMO LA STESSA COSA, INFINE, IN FORMATO PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF \\\n",
    ".write \\\n",
    ".mode(\"overwrite\") \\\n",
    ".partitionBy(\"shipvia\") \\\n",
    ".format(\"parquet\") \\\n",
    ".saveAsTable(\"spark.sample_orders_partition_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTROLLIAMO DA PROMPT DI SISTEMA OPERATIVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/hive/warehouse/spark.db/sample_orders_partition_parquet\n",
    "\n",
    "Found 5 items\n",
    "-rw-rw-r--   3 hdfs hive          0 2020-03-10 19:20 /user/hive/warehouse/spark.db/sample_orders_partition_parquet/_SUCCESS\n",
    "drwxrwxr-x   - hdfs hive          0 2020-03-10 19:20 /user/hive/warehouse/spark.db/sample_orders_partition_parquet/shipvia=1\n",
    "drwxrwxr-x   - hdfs hive          0 2020-03-10 19:20 /user/hive/warehouse/spark.db/sample_orders_partition_parquet/shipvia=2\n",
    "drwxrwxr-x   - hdfs hive          0 2020-03-10 19:20 /user/hive/warehouse/spark.db/sample_orders_partition_parquet/shipvia=3\n",
    "drwxrwxr-x   - hdfs hive          0 2020-03-10 19:20 /user/hive/warehouse/spark.db/sample_orders_partition_parquet/shipvia=__HIVE_DEFAULT_PARTITION__\n",
    "\n",
    "hdfs dfs -ls /user/hive/warehouse/spark.db/sample_orders_partition_parquet/shipvia=1\n",
    "\n",
    "Found 2 items\n",
    "-rw-rw-r--   3 hdfs hive      11898 2020-03-10 19:20 /user/hive/warehouse/spark.db/sample_orders_partition_parquet/shipvia=1/part-00000-809837c3-f972-412e-8a72-f726cc65f5c6.c000.snappy.parquet\n",
    "-rw-rw-r--   3 hdfs hive      11449 2020-03-10 19:20 /user/hive/warehouse/spark.db/sample_orders_partition_parquet/shipvia=1/part-00001-809837c3-f972-412e-8a72-f726cc65f5c6.c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE FINALI VARIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche se negli esempi visti non si e' potuto introdurre esempi relativamente al check dei NULL, si invita ad approfondire il concetto di \"is None\" e \"is not None\" ad esempio viene riportata una UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELIMINA LEADING E TRAILING BLANKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripBlanks(field_to_be_stripped):\n",
    "\tif (field_to_be_stripped is None):\n",
    "\t\tresultField = ''\n",
    "\telse:\n",
    "\t\tresultField = field_to_be_stripped.strip()\n",
    "\treturn (resultField)\n",
    "\n",
    "stpBlanks = udf(stripBlanks, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alla stessa stregua, molto importante e' la funzionalita' \"fillNa()\" quando si lavora con i Dataframes, laddove ad esempio non possiamo permetterci di avere campi nulli perche' provocherebbero problemi in fase di elaborazione. Si riporta ad esempio un comando per processare due colonne di un Dataframe, a nostra scelta, e sostituire in esse eventuali campi \"Null\" con degli \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESEMPIO DI USO DI \"fillNa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_DF.fillna(0, subset=['shipvia', 'orderid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un concetto molto importante da approfondire e' anche l'esecuzione di programmi Spark in modalita' \"Spark Streaming\", dove in pratica il nostro programma Python resta in ascolto in \"modalita' demone\" e permette di effettuare azioni live all'arrivo di generici eventi.\n",
    "\n",
    "Molto utile ad esempio in ambito IOT dove magari diversi Device comunicano periodicamente degli Update che vanno immediatamente processati ed ingeriti.\n",
    "\n",
    "Pensare anche a use case ambiziosi, tipo interfacciare il tutto con l'infrastruttura Kafka/Kafka Connect in modo da poter essere scalabili senza alcun problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALLA FINE DELL'ESERCIZIO ENTRIAMO IN HIVE E FACCIAMO UN PO' DI PULIZIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DROP TABELLE DI DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop table categories;\n",
    "drop table customers;\n",
    "drop table employee_territories;\n",
    "drop table employees;\n",
    "drop table order_details;\n",
    "drop table orders;\n",
    "drop table products;\n",
    "drop table regions;\n",
    "drop table shippers;\n",
    "drop table suppliers;\n",
    "drop table territories;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
